  Given the rigid coordination of steps in this multi-agent system, I opted to use a workflow approach to sequentially order the research process. This organization simplifies task specification and ordering, enabling the system to scale as more tasks and data sources are added. Using the [existing multi-agent workflow](https://github.com/strands-agents/docs/blob/main/docs/examples/python/agents_workflows.md) as an example, I built on top of this functionality to add a bibliography feature for clear citation references and added a RAG search to retrieve user or session specific information from local text files. Using this local data storage, the user is able to contribute their own documentation and relevant research papers for investigtion. Similarly, given more time, I would aim to add retrieval from the Bedrock Knowledge Base, storing acquired research data in the KB to pull from in agentic responses.
  As more task-specific agents are added, agent-level and task-level prompts can be abstracted to their own separate configuration files. Additionally, rather than using my manually built 'y/n' consent question, I would like to migrate to human-in-the-loop validation, using the existing interrupt system in the Strands SDK.
